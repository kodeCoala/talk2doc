{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b1124100-d365-4627-81da-2bafc22384bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "134789ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows nested access to event loop\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "402ba272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/msrivas/hack/ai/talk2doc/docs\n"
     ]
    }
   ],
   "source": [
    "# cohere API key\n",
    "API_KEY = os.environ.get(\"API_KEY\")\n",
    "# input doc directory\n",
    "input_dir_name = os.environ.get(\"DIR\")\n",
    "print(input_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f62f3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = SimpleDirectoryReader(\n",
    "  input_dir=input_dir_name,\n",
    "  required_exts=['.pdf'],\n",
    "  recursive=True\n",
    ")\n",
    "\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "136ac929",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = CohereEmbedding(\n",
    "  cohere_api_key=API_KEY,\n",
    "  model_name='embed-english-v3.0',\n",
    "  input_type='search_query'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ceebfd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create vecotr store and upload indexed data\n",
    "Settings.embed_model = embed_model\n",
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "45912a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "# create query engine\n",
    "cohere_rerank = CohereRerank(api_key=API_KEY, top_n=2)\n",
    "Settings.llm = Cohere(api_key=API_KEY, model='command-r-plus')\n",
    "query_engine = index.as_query_engine(node_postprocessors=[cohere_rerank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1907cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "qa_prompt_tmpl_str = (\n",
    "  \"Context information is below.\\n\"\n",
    "  \"-------------------------\\n\"\n",
    "  \"{context_str}\\n\"\n",
    "  \"Given the context information above I want you\\n\"\n",
    "  \"to think step by step to answer the query in a crisp \\n\"\n",
    "  \"manner, incase you don't knkow the answer say \\n\"\n",
    "  \"'I don't know!'. \\n\"\n",
    "  \"Query: {query_str}\\n\"\n",
    "  \"Answer: \"\n",
    ")\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
    "question = 'explain do VAE\\'s work in detail. Explain in steps how one would go about building a VAE'\n",
    "response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cf478f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can provide a high-level explanation of how VAEs work and a step-by-step guide\n",
      "on building one.   **How VAEs Work:**  VAEs (Variational Autoencoders) are a\n",
      "type of generative model that can learn to generate new data similar to the\n",
      "training data. They do this by learning a probability distribution that captures\n",
      "the underlying patterns in the data. Here's a simplified step-by-step\n",
      "explanation:  1. **Encoding:** The VAE first encodes the input data x into a\n",
      "latent representation z using an encoder network qφ(z|x). This latent space z is\n",
      "typically of lower dimension than the input space, and it captures the most\n",
      "important features of the data.  2. **Sampling:** The VAE then samples new data\n",
      "points from the latent space z. This is done by using a prior distribution\n",
      "pθ(z), which is often a simple distribution like a spherical Gaussian.  3.\n",
      "**Decoding:** The sampled latent vectors z are then passed through a decoder\n",
      "network pθ(x|z), which generates new data points x' that resemble the training\n",
      "data. The decoder tries to reconstruct the original input data from the latent\n",
      "representation.  4. **Loss Function:** The VAE is trained using a loss function\n",
      "that has two main components:     a. **Reconstruction Loss:** This measures how\n",
      "well the VAE can reconstruct the original input data x from the latent\n",
      "representation z. It encourages the decoder to generate data that looks like the\n",
      "training data.     b. **Kullback-Leibler (KL) Divergence:** This term acts as a\n",
      "regularizer and encourages the distribution of the latent space qφ(z|x) to be\n",
      "similar to the prior distribution pθ(z). This helps in avoiding overfitting and\n",
      "keeps the latent space smooth and continuous.   **Building a VAE:**  Here's a\n",
      "step-by-step guide on how one might go about building a VAE:  1. **Define the\n",
      "Model Architecture:** Decide on the structure of your encoder and decoder\n",
      "networks. The encoder takes the input data x and outputs the parameters of the\n",
      "latent distribution qφ(z|x). The decoder takes samples from the latent\n",
      "distribution and outputs the generated data x'.  2. **Choose the Prior\n",
      "Distribution:** Select a suitable prior distribution pθ(z) for the latent space.\n",
      "A common choice is a standard Gaussian distribution.  3. **Define the Loss\n",
      "Function:** Implement the VAE loss function, which includes the reconstruction\n",
      "loss and the KL divergence term.  4. **Train the Model:** Use a suitable dataset\n",
      "to train the VAE. During training, the encoder and decoder networks learn to\n",
      "optimize the loss function. This is typically done using gradient descent and\n",
      "backpropagation.  5. **Sample and Generate:** After training, you can use the\n",
      "encoder to map new data points to the latent space and then sample from this\n",
      "latent space to generate new, similar data points using the decoder.  6.\n",
      "**Evaluate and Tweak:** Evaluate the quality of the generated data using\n",
      "appropriate metrics and visualizations. You may need to adjust the model\n",
      "architecture, hyperparameters, or training data to improve the results.\n",
      "Remember that this is a simplified explanation, and building a successful VAE\n",
      "often involves a lot of experimentation and fine-tuning. The specific\n",
      "implementation details can vary depending on the programming language and\n",
      "libraries you use.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "wrapped_text = textwrap.fill(str(response), width=80)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f8bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talk2doc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
